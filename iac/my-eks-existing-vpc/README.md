# EKS Cluster com VPC Existente

## ðŸ“‹ VisÃ£o Geral

Este projeto provisiona um **cluster Amazon EKS** usando uma **VPC e subnets existentes**. Ã‰ uma versÃ£o otimizada que nÃ£o cria recursos de rede, apenas o cluster EKS e os worker nodes.

## ðŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       VPC EXISTENTE (10.0.0.0/16)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AZ us-east-1a                    â”‚  AZ us-east-1c                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚ Subnet Privada EXISTENTE            â”‚ Subnet Privada EXISTENTE                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚ â”‚ âš™ï¸  EKS Control Plane   â”‚         â”‚ â”‚ âš™ï¸  EKS Control Plane    â”‚           â”‚
â”‚ â”‚ ðŸ–¥ï¸  Worker Nodes (2-10) â”‚         â”‚ â”‚ ðŸ–¥ï¸  Worker Nodes (2-10)  â”‚           â”‚
â”‚ â”‚     t3.large instances  â”‚         â”‚ â”‚     t3.large instances   â”‚           â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”§ Componentes Provisionados

### âœ… **O que SERÃ criado:**
- âš™ï¸ **EKS Control Plane** (Kubernetes 1.28)
- ðŸ–¥ï¸ **EKS Node Group** (t3.large, Auto Scaling 2-10)
- ðŸ”’ **IAM Roles** (Cluster + Worker Nodes)
- ðŸ›¡ï¸ **Security Groups** (Control Plane)

### âŒ **O que NÃƒO serÃ¡ criado:**
- ðŸŒ VPC (usa existente)
- ðŸ”Œ Subnets (usa existentes)
- ðŸšª Internet Gateway (usa existente)
- ðŸ“¡ NAT Gateway (usa existente)
- ðŸ›£ï¸ Route Tables (usa existentes)

## ðŸ“ Estrutura do Projeto

```
â”œâ”€â”€ main.tf                 # ConfiguraÃ§Ã£o principal com data sources
â”œâ”€â”€ provider.tf             # AWS Provider
â”œâ”€â”€ variables.tf            # VariÃ¡veis (incluindo IDs de recursos existentes)
â”œâ”€â”€ outputs.tf              # Outputs importantes do cluster
â”œâ”€â”€ terraform.tfvars        # Valores das variÃ¡veis (personalizar)
â””â”€â”€ modules/
    â”œâ”€â”€ master/             # MÃ³dulo EKS Control Plane
    â”‚   â”œâ”€â”€ master.tf       # Cluster EKS
    â”‚   â”œâ”€â”€ iam.tf          # IAM Roles para cluster
    â”‚   â”œâ”€â”€ security.tf     # Security Groups
    â”‚   â”œâ”€â”€ variables.tf    # VariÃ¡veis do mÃ³dulo
    â”‚   â””â”€â”€ outputs.tf      # Outputs do cluster
    â””â”€â”€ nodes/              # MÃ³dulo Worker Nodes
        â”œâ”€â”€ node_group.tf   # EKS Node Groups
        â”œâ”€â”€ iam.tf          # IAM Roles para nodes
        â”œâ”€â”€ variables.tf    # VariÃ¡veis do mÃ³dulo
        â””â”€â”€ outputs.tf      # Outputs dos nodes
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. **Personalizar terraform.tfvars**

```hcl
# Nome do cluster
cluster_name = "meu-eks-cluster"

# RegiÃ£o AWS
aws_region = "us-east-1"

# IDs dos recursos existentes (OBRIGATÃ“RIO - substituir pelos seus)
vpc_id = "vpc-xxxxxxxxx"
private_subnet_ids = [
  "subnet-xxxxxxxxx",  # us-east-1a
  "subnet-yyyyyyyyy"   # us-east-1c
]

# ConfiguraÃ§Ã£o do cluster
k8s_version = "1.28"
nodes_instances_sizes = ["t3.large"]
auto_scale_options = {
  min     = 2
  max     = 10
  desired = 2
}
```

### 2. **Obter IDs dos Recursos Existentes**

```bash
# Listar VPCs
aws ec2 describe-vpcs --query 'Vpcs[*].[VpcId,Tags[?Key==`Name`].Value|[0],CidrBlock]' --output table

# Listar subnets privadas
aws ec2 describe-subnets \
  --filters "Name=vpc-id,Values=vpc-xxxxxxxxx" "Name=tag:Name,Values=*private*" \
  --query 'Subnets[*].[SubnetId,AvailabilityZone,CidrBlock,Tags[?Key==`Name`].Value|[0]]' \
  --output table
```

## ðŸš€ Deploy

### PrÃ©-requisitos
- Terraform >= 1.3
- AWS CLI configurado
- VPC e subnets privadas jÃ¡ existentes
- Credenciais AWS com permissÃµes para EKS

### Deploy do Cluster

```bash
# 1. Entrar na pasta do projeto
cd /home/hugo/ambiente/Kubernetes/iac/my-eks-existing-vpc

# 2. Personalizar terraform.tfvars com seus recursos existentes
cp terraform.tfvars.example terraform.tfvars
# Editar terraform.tfvars com os IDs corretos

# 3. Inicializar Terraform
terraform init

# 4. Validar configuraÃ§Ã£o
terraform validate

# 5. Planejar deployment
terraform plan

# 6. Aplicar infraestrutura
terraform apply
```

### Conectar ao Cluster

```bash
# Configurar kubectl
aws eks update-kubeconfig --region us-east-1 --name meu-eks-cluster

# Verificar nodes
kubectl get nodes

# Verificar status
kubectl cluster-info
```

## ðŸ“Š Vantagens desta Abordagem

### âœ… **BenefÃ­cios:**
- ðŸš€ **Deploy mais rÃ¡pido** (nÃ£o cria rede)
- ðŸ’° **Evita custos duplicados** (reutiliza NAT Gateway)
- ðŸ”„ **Reutiliza infraestrutura** existente
- ðŸ›¡ï¸ **MantÃ©m configuraÃ§Ãµes de rede** jÃ¡ testadas
- ðŸ“¦ **Modular** (pode ser destruÃ­do sem afetar a rede)

### âš ï¸ **ConsideraÃ§Ãµes:**
- ðŸ” **Requer IDs corretos** (VPC e subnets)
- ðŸ·ï¸ **Tags podem ser necessÃ¡rias** para subnets
- ðŸ” **PermissÃµes de rede** devem estar corretas

## ðŸ”’ Tags NecessÃ¡rias nas Subnets

Para o EKS funcionar corretamente, as subnets devem ter estas tags:

```bash
# Subnets privadas (onde ficam os worker nodes)
kubernetes.io/role/internal-elb = 1
kubernetes.io/cluster/[CLUSTER_NAME] = owned

# Subnets pÃºblicas (se existirem - para load balancers)
kubernetes.io/role/elb = 1
kubernetes.io/cluster/[CLUSTER_NAME] = owned
```

### Aplicar Tags nas Subnets:

```bash
# Substituir pelos seus valores
CLUSTER_NAME="meu-eks-cluster"
PRIVATE_SUBNET_1="subnet-xxxxxxxxx"
PRIVATE_SUBNET_2="subnet-yyyyyyyyy"

# Tags para subnets privadas
aws ec2 create-tags --resources $PRIVATE_SUBNET_1 $PRIVATE_SUBNET_2 --tags \
  Key=kubernetes.io/role/internal-elb,Value=1 \
  Key=kubernetes.io/cluster/$CLUSTER_NAME,Value=owned
```

## ðŸ§¹ Limpeza

```bash
# Destruir apenas o cluster (mantÃ©m a rede)
terraform destroy

# A VPC e subnets permanecerÃ£o intactas
```

## ðŸŽ¯ Status do Projeto

### âœ… **Funcionalidades:**
- âœ… Cluster EKS 1.28
- âœ… Worker Nodes com Auto Scaling
- âœ… Usa VPC existente
- âœ… IAM configurado
- âœ… Security Groups otimizados
- âœ… Outputs informativos

### ðŸ”„ **PrÃ³ximos Passos:**
1. Deploy do cluster
2. Instalar Ingress Controller
3. Deploy das aplicaÃ§Ãµes
4. Configurar monitoramento

---
**VersÃ£o:** 1.0  
**Terraform:** >= 1.3  
**Kubernetes:** 1.28  
**AWS Provider:** ~> 5.0  
